---
title: "Book Recommender System"
author: "Tino Muzambi"
format: 
  html:
    embed-resources: true
    page-layout: full
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
# Libraries
install.if.missing <- function(package.name) {
  if (!require(package.name, character.only = TRUE)) {
    install.packages(package.name)
    library(package.name, character.only = TRUE)
  }
}

install.if.missing("tidyverse")
install.if.missing("recosystem")
```

# The Data

The data we will be working with is a partially preprocessed version of the ”Book-Crossing” dataset. The data can be downloaded [here](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/) from Kaggle. It was collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the Book-Crossing community with kind permission from Ron Hornbaker, CTO of Humankind Systems. It contains 278 858 users (anonymised but with demographic information) providing 1 149 780 ratings (explicit/implicit) about 271 379 books.

```{r}
# Load the data
ratings <- read.csv("data/Ratings.csv")
books <- read.csv("data/Books.csv")
users <- read.csv("data/Users.csv")

head(ratings)
head(books)
head(users)
```

# Data Pre-processing

We will merge the data into a single dataframe then perform various functions to reduce the number of observations for computational efficiency. We will also perform some data cleaning. We will also recode the user and item IDs to be sequential integers starting from 0 for recosystem later.
```{r}
# Merge the data
comb.ratings <- ratings %>% 
  left_join(books, by = "ISBN") %>% 
  left_join(users, by = "User.ID")
rm(ratings, books, users)

# Recode the user and item IDs
user.ids <- data.frame(User.ID = unique(comb.ratings$User.ID), new.user.id = 0:(length(unique(comb.ratings$User.ID)) - 1))
book.ids <- data.frame(ISBN = unique(comb.ratings$ISBN), new.book.id = 0:(length(unique(comb.ratings$ISBN)) - 1))

# Update the user and item IDs
comb.ratings <- comb.ratings %>% 
  left_join(user.ids) %>% 
  left_join(book.ids) %>% 
  select(User.ID = new.user.id, ISBN = new.book.id, Book.Rating, Year.Of.Publication, Age, Book.Title)

rm(book.ids)
rm(user.ids)
```

Explore the structure of the combined data.

```{r}
str(comb.ratings)

summary(comb.ratings)
```

Looking at the data, we identify some cleaning that needs to be done. We will do the following to clean up the data:

-   Remove the columns that we do not need

-   Replace ages less than 13 and greater than 100 with NA

-   Convert the data types of the columns to the appropriate types

-   Replace year of publication with NA where it is before 1900

-   Remove any rows with missing values

```{r}
# Perform data cleaning
comb.ratings <- comb.ratings %>% 
  mutate(Age = ifelse(Age < 13 | Age > 100, NA, Age)) %>% 
  mutate(Year.Of.Publication = ifelse(Year.Of.Publication < 1900, NA, Year.Of.Publication)) %>% 
  mutate(Year.Of.Publication = as.numeric(Year.Of.Publication)) %>% 
  drop_na()
summary(comb.ratings)
```

Preprocess the data to remove users with less than 5 ratings and books with less than 5 ratings
```{r}
preprocess.data <- function(ratings, min.ratings = 5) {
  user.counts <- table(ratings$User.ID)
  valid.users <- names(user.counts[user.counts >= min.ratings])
  
  filtered.ratings <- ratings %>%
    filter(User.ID %in% valid.users) %>%
    group_by(ISBN) %>%
    filter(n() >= min.ratings) %>%
    ungroup()
  
  return(filtered.ratings)
}
```

# Exploratory Data Analysis
We will now perform some exploratory data analysis to understand the data better.

```{r}
# Plot the distribution of ratings, age and year of publication faceted
comb.ratings %>% 
  gather(key = "key", value = "value", Book.Rating, Age, Year.Of.Publication) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Key Variables", x = "Value", y = "Count") +
  facet_wrap(~key, scales = "free") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```
We observe a large number of zero ratings. This can be attributed to the fact that a majority of the users posting reviews are doing so because they had a bad experience. Alternatively the users, or the site the data is derived from, could be configured to automatically input a 0 for the book rating. Before building the recommendation system, we will only keep the columns we need for the analysis. We will also then take a sample of 35 000 ratings for computational efficiency.
```{r}
comb.ratings <- preprocess.data(comb.ratings)

# Select the columns we need
comb.ratings <- comb.ratings %>% 
  select(User.ID, ISBN, Book.Rating, Book.Title)

# Sample 35 000 ratings
set.seed(50731)
comb.ratings <- comb.ratings[sample(nrow(comb.ratings), 35000),]
```

# Recommendation Systems
We will now begin building our recommendation system. We will start by building a user-based collaborative filtering system. We will then build an item-based collaborative filtering system. Finally, we will build a matrix factorisation system.

## User-Based Collaborative Filtering
We will start by building a user-based collaborative filtering system. We will first create a user-item matrix then calculate the cosine similarity between users. We will then use the similarity matrix to predict the ratings for the books.
```{r}
create.ratings.matrix <- function(ratings.data) {
  ratings.matrix <- ratings.data %>%
    select(User.ID, ISBN, Book.Rating) %>%
    pivot_wider(names_from = ISBN, values_from = Book.Rating) %>%
    column_to_rownames(var = "User.ID")
  
  return(as.matrix(ratings.matrix))
}
```


```{r}
cosine.similarity <- function(vector1, vector2) {
  # Filter out NA values
  vector1 <- vector1[!is.na(vector1)]
  vector2 <- vector2[!is.na(vector2)]
  
  # Find common items
  common.items <- intersect(names(vector1), names(vector2))
  
  # If there are no common items, return 0 (no similarity)
  if (length(common.items) == 0) return(0)
  
  # Subset both vectors to only include common items
  vector1 <- vector1[common.items]
  vector2 <- vector2[common.items]
  
  # Calculate dot product
  dot.product <- sum(vector1 * vector2)
  
  # Calculate magnitudes
  magnitude1 <- sqrt(sum(vector1^2))
  magnitude2 <- sqrt(sum(vector2^2))
  
  # Avoid division by zero
  if (magnitude1 == 0 || magnitude2 == 0) return(0)
  
  # Calculate cosine similarity
  similarity <- dot.product / (magnitude1 * magnitude2)
  
  return(similarity)
}
```

```{r}
predict.rating.ubcf <- function(user.id, isbn, r.matrix, k = 10) {
  user.id <- as.character(user.id)
  isbn <- as.character(isbn)
  
  if (!(isbn %in% colnames(r.matrix))) return(NA)
  if (!(user.id %in% rownames(r.matrix))) return(NA)
  
  user.ratings <- r.matrix[user.id, ]
  other.users <- r.matrix[rownames(r.matrix) != user.id, ]
  
  similarities <- apply(other.users, 1, function(x) cosine.similarity(user.ratings, x))
  nearest.neighbors <- head(sort(similarities, decreasing = TRUE), k)
  
  if (all(is.na(nearest.neighbors))) return(0)
  
  weighted.ratings <- nearest.neighbors * r.matrix[names(nearest.neighbors), isbn]
  predicted.rating <- sum(weighted.ratings, na.rm = TRUE) / sum(nearest.neighbors[!is.na(weighted.ratings)])
  
  if (is.na(predicted.rating)) return(0)
  
  return(predicted.rating)
}
```

## Item-Based Collaborative Filtering
We will now build an item-based collaborative filtering system by calculating the cosine similarity between items. We will then use the similarity matrix to predict the ratings for the books.
```{r}
predict.rating.ibcf <- function(user.id, isbn, r.matrix, k = 10) {
  user.id <- as.character(user.id)
  isbn <- as.character(isbn)
  
  if (!(user.id %in% rownames(r.matrix))) return(NA)
  if (!(isbn %in% colnames(r.matrix))) return(NA)
  
  user.ratings <- r.matrix[user.id, ]
  other.items <- r.matrix[, colnames(r.matrix) != isbn]
  
  similarities <- apply(other.items, 2, function(x) cosine.similarity(r.matrix[, isbn], x))
  nearest.neighbors <- head(sort(similarities, decreasing = TRUE), k)
  
  if (all(is.na(nearest.neighbors))) return(0)
  
  weighted.ratings <- nearest.neighbors * user.ratings[names(nearest.neighbors)]
  predicted.rating <- sum(weighted.ratings, na.rm = TRUE) / sum(nearest.neighbors[!is.na(weighted.ratings)])
  
  if (is.na(predicted.rating)) return(0)
  
  return(predicted.rating)
}
```

## Matrix Factorisation
We will now build a matrix factorisation system using recosystem. We will first allocate training and test splits. We will then set up the model and make predictions on the test set.
```{r}
# Create test/train split.
test.data <- comb.ratings %>% 
  group_by(User.ID) %>%
  slice_sample(prop = 0.2) %>%
  ungroup()
train.data <- comb.ratings %>% 
  anti_join(test.data, by = c("User.ID", "ISBN"))
```

```{r}
# Set up the Recosystem params.
reco.train <- data_memory(
  user_index = train.data$User.ID,
  item_index = train.data$ISBN,
  rating = train.data$Book.Rating
)

reco.test <- data_memory(
  user_index = test.data$User.ID,
  item_index = test.data$ISBN,
  rating = test.data$Book.Rating
)
```

```{r}
# Set up the model.
rs <- Reco()

rs$train(reco.train, opts = list(
  dim = 10,
  nmf = T,
  niter = 50,
  verbose = F
))
```

```{r}
# Hyperparameter tuning
opts <- rs$tune(reco.train, opts = list(
  dim = c(50, 75, 100),
  lrate = c(0.001, 0.005, 0.01),
  niter = 20,
  nmf = T,
  nthread = 4
))
opts
```

```{r}
# Retrain the model with the optimal hyperparameters
rs$train(reco.train, opts = opts)
```

Let's do some predictions
```{r}
# Create ratings matrix
ratings.matrix <- create.ratings.matrix(comb.ratings)

# Predictions
predict.rating.ubcf(59494, 616, ratings.matrix)
predict.rating.ibcf(59494, 616, ratings.matrix)
mf.predict <- rs$predict(reco.test)
```

We will now evaluate the accuracy of the matrix factorisation model using RMSE.
```{r}
# Calculate RMSE
sqrt(mean((mf.predict - test.data$Book.Rating)^2))
```

Apply regularisation to the matrix factorisation model.
```{r}
rs$train(reco.train, opts = list(
  dim = 10,
  nmf = T,
  niter = 50,
  verbose = F,
  lambda = 0.01
))

mf.predict <- rs$predict(reco.test)
sqrt(mean((mf.predict - test.data$Book.Rating)^2))
```


We will now create an ensemble model that combines the user-based collaborative filtering, item-based collaborative filtering, and matrix factorisation models. We will then evaluate the accuracy of the ensemble model using RMSE.
```{r}
ensemble.predict <- function(user.id, isbn, r.matrix = ratings.matrix) {
  ubcf.prediction <- predict.rating.ubcf(user.id, isbn, r.matrix)
  ibcf.prediction <- predict.rating.ibcf(user.id, isbn, r.matrix)
  mf.prediction <- rs$predict(data_memory(user_index = user.id, item_index = isbn))
  
  return((ubcf.prediction + ibcf.prediction + mf.prediction) / 3)
}
```

Test the ensemble model
```{r}
ensemble.predict(80, 433)
```

Evaluate the accuracy of the ensemble model using RMSE.
```{r}
ensemble.predictions <- mapply(ensemble.predict, test.data$User.ID, test.data$ISBN)
sqrt(mean((ensemble.predictions - test.data$Book.Rating)^2))
```

