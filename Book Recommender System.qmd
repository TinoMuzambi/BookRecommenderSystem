---
title: "Book Recommender System"
execute:
  include: false
author: "Tino Muzambi"
format: 
  html:
    embed-resources: true
    page-layout: full
    toc: true
  pdf: default
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, include=FALSE}
# Libraries
install.if.missing <- function(package.name) {
  if (!require(package.name, character.only = TRUE)) {
    install.packages(package.name)
    library(package.name, character.only = TRUE)
  }
}

install.if.missing("tidyverse")
install.if.missing("recosystem")
```

# Introduction
In this project, we will build a book recommendation system using collaborative filtering and matrix factorisation. We will start by loading the data and performing some data pre-processing. We will then build a user-based collaborative filtering system, an item-based collaborative filtering system, and a matrix factorisation system. We will evaluate the accuracy of the models using RMSE. Finally, we will create an ensemble model that combines the user-based collaborative filtering, item-based collaborative filtering, and matrix factorisation models.

# The Data
The data is a partially preprocessed version of the ”Book-Crossing” dataset consisting of three datasets: Ratings, Books, and Users. The Ratings dataset contains the ratings given by users to books. The Books dataset contains information about the books, such as the title and year of publication. The Users dataset contains information about the users, such as their age. The data can be downloaded [here](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/) from Kaggle. It contains 278 858 users providing 1 149 780 ratings about 271 379 books.

```{r, echo=FALSE}
# Load the data
ratings <- read.csv("data/Ratings.csv")
books <- read.csv("data/Books.csv")
users <- read.csv("data/Users.csv")

head(ratings)
head(books)
head(users)
```

# Data Pre-processing
We will merge the books, ratings and user data into a single dataframe. This will make working with the data easier. Due to the large size of the data, we will then perform various functions to reduce the number of observations. These will include:
-   Removing users with less than 5 ratings.
-   Removing books with less than 5 ratings.
-   Sampling 35 000 ratings for computational efficiency.

We will also perform some data cleaning. Part of which includes recoding the user and book IDs to be sequential integers starting from 0. This is necessary for the `recosystem` package which will be used in the matrix factorisation model.
```{r, echo=FALSE}
# Merge the data
comb.ratings <- ratings %>% 
  left_join(books, by = "ISBN") %>% 
  left_join(users, by = "User.ID")
rm(ratings, books, users)
```

```{r}
# Recode the user and book IDs
user.ids <- data.frame(User.ID = unique(comb.ratings$User.ID), new.user.id = 0:(length(unique(comb.ratings$User.ID)) - 1))
book.ids <- data.frame(ISBN = unique(comb.ratings$ISBN), new.book.id = 0:(length(unique(comb.ratings$ISBN)) - 1))

# Update the user and book IDs
comb.ratings <- comb.ratings %>% 
  left_join(user.ids) %>% 
  left_join(book.ids) %>% 
  select(User.ID = new.user.id, ISBN = new.book.id, Book.Rating, Year.Of.Publication, Age, Book.Title)

rm(book.ids)
rm(user.ids)
```

```{r, echo=FALSE}
# Preprocess the data to remove users with less than 5 ratings and books with less than 5 ratings, drop zero ratings and select only the columns we need.
preprocess.data <- function(ratings, min.ratings = 5) {
  user.counts <- table(ratings$User.ID)
  valid.users <- names(user.counts[user.counts >= min.ratings])
  
  filtered.ratings <- ratings %>%
    filter(User.ID %in% valid.users) %>%
    group_by(ISBN) %>%
    filter(n() >= min.ratings) %>%
    ungroup() %>%
  filter(Book.Rating != 0) %>% 
  select(User.ID, ISBN, Book.Rating, Book.Title)
  
  return(filtered.ratings)
}
```

# Exploratory Data Analysis
We will now perform some exploratory data analysis to understand the data better.

```{r, echo=FALSE}
# Structure and summary of the data
str(comb.ratings)

summary(comb.ratings)
```

Looking at the data, we identify some cleaning that needs to be done. We will do the following to clean up the data:

-   Remove the columns that we do not need.

-   Replace ages less than 13 and greater than 100 with NA.

-   Convert the data types of the columns to the appropriate types.

-   Replace year of publication with NA where it is before 1900.

-   Remove any rows with missing values.

```{r, echo=FALSE}
# Perform data cleaning
comb.ratings <- comb.ratings %>% 
  mutate(Age = ifelse(Age < 13 | Age > 100, NA, Age)) %>% 
  mutate(Year.Of.Publication = ifelse(Year.Of.Publication < 1900, NA, Year.Of.Publication)) %>% 
  mutate(Year.Of.Publication = as.numeric(Year.Of.Publication)) %>% 
  drop_na()
summary(comb.ratings)
```

We will look at the distribution of the ratings, age, and year of publication to understand the data better. We will use histograms to visualise the distributions.
```{r, echo=FALSE}
# Distribution of key variables
comb.ratings %>% 
  gather(key = "key", value = "value", Book.Rating, Age, Year.Of.Publication) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Key Variables", x = "Value", y = "Count") +
  facet_wrap(~key, scales = "free") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```
We observe a large number of zero ratings. This can be attributed to the fact that a majority of the users posting reviews are doing so because they had a bad experience. Alternatively the users, or the site the data is derived from, could be configured to automatically input a 0 for the book rating. We will drop the zero ratings to improve the accuracy of the recommendation systems.

Before building the recommendation systems, we will drop users and books with less than five ratings and only keep the columns we need for the analysis. Namely, the user ID, ISBN, book rating and the book title. We will also then take a sample of 35 000 ratings for computational efficiency.
```{r, echo=FALSE}
# Preprocess the data
comb.ratings <- preprocess.data(comb.ratings)

# Sample 35 000 ratings
set.seed(50731)
comb.ratings <- comb.ratings[sample(nrow(comb.ratings), 35000),]
```

We will now look at the distribution of the ratings after removing the zero ratings.
```{r}
# Distribution of ratings
comb.ratings %>%
  ggplot(aes(x = Book.Rating)) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Distribution of Ratings", x = "Rating", y = "Count") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```
The distribution of the ratings shows that most of the ratings are between 5 and 10. This is expected as users are more likely to rate books that they enjoyed.

# Recommendation Systems
We will now begin building our recommendation systems. We will start by building a user-based collaborative filtering system. We will then build an item-based collaborative filtering system. Finally, we will build a matrix factorisation system before creating an ensemble model that combines the three models.

## User-Based Collaborative Filtering
We will start by building a user-based collaborative filtering system. User based collaborative filtering finds users with similar consumption patterns as yourself and gives you the content that these similar users found interesting.[@bostrom2017comparison] How it works is that it first creates a user-item matrix. The user-item matrix is a matrix where the rows represent users, the columns represent items, and the cells represent the ratings given by users to items. This matrix is then used to calculate the cosine similarity between users. We will then use the similarity matrix to predict the ratings for the books.
```{r, echo=FALSE}
create.ratings.matrix <- function(ratings.data) {
  ratings.matrix <- ratings.data %>%
    select(User.ID, ISBN, Book.Rating) %>%
    pivot_wider(names_from = ISBN, values_from = Book.Rating) %>%
    column_to_rownames(var = "User.ID")
  
  return(as.matrix(ratings.matrix))
}
```

```{r, echo=FALSE}
cosine.similarity <- function(vector1, vector2) {
  # Filter out NA values
  vector1 <- vector1[!is.na(vector1)]
  vector2 <- vector2[!is.na(vector2)]
  
  # Find common items
  common.items <- intersect(names(vector1), names(vector2))
  
  # If there are no common items, return 0 (no similarity)
  if (length(common.items) == 0) return(0)
  
  # Subset both vectors to only include common items
  vector1 <- vector1[common.items]
  vector2 <- vector2[common.items]
  
  # Calculate dot product
  dot.product <- sum(vector1 * vector2)
  
  # Calculate magnitudes
  magnitude1 <- sqrt(sum(vector1^2))
  magnitude2 <- sqrt(sum(vector2^2))
  
  # Avoid division by zero
  if (magnitude1 == 0 || magnitude2 == 0) return(0)
  
  # Calculate cosine similarity
  similarity <- dot.product / (magnitude1 * magnitude2)
  
  return(similarity)
}
```

```{r, echo=FALSE}
predict.rating.ubcf <- function(user.id, isbn, r.matrix, k = 10) {
  user.id <- as.character(user.id)
  isbn <- as.character(isbn)
  
  if (!(isbn %in% colnames(r.matrix))) return(NA)
  if (!(user.id %in% rownames(r.matrix))) return(NA)
  
  user.ratings <- r.matrix[user.id, ]
  other.users <- r.matrix[rownames(r.matrix) != user.id, ]
  
  similarities <- apply(other.users, 1, function(x) cosine.similarity(user.ratings, x))
  nearest.neighbors <- head(sort(similarities, decreasing = TRUE), k)
  
  if (all(is.na(nearest.neighbors))) return(0)
  
  weighted.ratings <- nearest.neighbors * r.matrix[names(nearest.neighbors), isbn]
  predicted.rating <- sum(weighted.ratings, na.rm = TRUE) / sum(nearest.neighbors[!is.na(weighted.ratings)])
  
  if (is.na(predicted.rating)) return(0)
  
  return(predicted.rating)
}
```

## Item-Based Collaborative Filtering
We will now build an item-based collaborative filtering system. Item-based collaborative filtering uses similarity between the items to determine whether a user would like it or not. The same matrix created in the user-based collaborative filtering will be used. This matrix is used to calculate the cosine similarity between items. We will then use the similarity matrix to predict the ratings for the books.
```{r, echo=FALSE}
predict.rating.ibcf <- function(user.id, isbn, r.matrix, k = 10) {
  user.id <- as.character(user.id)
  isbn <- as.character(isbn)
  
  if (!(user.id %in% rownames(r.matrix))) return(NA)
  if (!(isbn %in% colnames(r.matrix))) return(NA)
  
  user.ratings <- r.matrix[user.id, ]
  other.items <- r.matrix[, colnames(r.matrix) != isbn]
  
  similarities <- apply(other.items, 2, function(x) cosine.similarity(r.matrix[, isbn], x))
  nearest.neighbors <- head(sort(similarities, decreasing = TRUE), k)
  
  if (all(is.na(nearest.neighbors))) return(0)
  
  weighted.ratings <- nearest.neighbors * user.ratings[names(nearest.neighbors)]
  predicted.rating <- sum(weighted.ratings, na.rm = TRUE) / sum(nearest.neighbors[!is.na(weighted.ratings)])
  
  if (is.na(predicted.rating)) return(0)
  
  return(predicted.rating)
}
```

## Matrix Factorisation
We will now build a matrix factorisation system using recosystem. Matrix factorization is an extensively used technique in collaborative filtering recommendation systems. Its objective is to factorise a user-item matrix into two low-ranked matrices, the user-factor matrix and the item-factor matrix, that can predict new items that users might be interested in. This is achieved in by multiplying the two factor matrices.[@isinkaye2023matrix]

We will first allocate training and test splits. We will then set up the model. We will also perform some hyperparameter tuning to find the best values for the learning rate and dim. We will then make predictions on the test set. We will evaluate the accuracy of the model using RMSE. We will then apply regularisation to the model and evaluate the accuracy again.
```{r, echo=FALSE}
# Create test/train split.
set.seed(50731)
test.data <- comb.ratings %>% 
  group_by(User.ID) %>%
  slice_sample(prop = 0.2) %>%
  ungroup()

train.data <- comb.ratings %>% 
  anti_join(test.data, by = c("User.ID", "ISBN"))
```

```{r, echo=FALSE}
# Set up the Recosystem params.
reco.train <- data_memory(
  user_index = train.data$User.ID,
  item_index = train.data$ISBN,
  rating = train.data$Book.Rating
)

reco.test <- data_memory(
  user_index = test.data$User.ID,
  item_index = test.data$ISBN,
  rating = test.data$Book.Rating
)
```

```{r, echo=FALSE}
# Set up the model.
rs <- Reco()

rs$train(reco.train, opts = list(
  dim = 10,
  lrate = 0.1,
  niter = 50,
  nthread = 4,
  verbose = F,
  nmf = T,
  costp_l1 = 0, costq_l1 = 0,
  costp_l2 = 0, costq_l2 = 0
))
```

### Hyperparameter Tuning
```{r, echo=FALSE}
# Hyperparameter tuning
set.seed(50731)
opts <- rs$tune(reco.train, opts = list(
  dim = c(50, 75, 100),
  lrate = c(0.1, 0.25, 0.5),
  niter = 50,
  nmf = T,
  verbose = F,
  nthread = 4,
  costp_l1 = 0, costq_l1 = 0,
  costp_l2 = 0, costq_l2 = 0
))
opts
```
The initial grid for the hyperparameter search involved varying `dim` from 0 - 50 and `lrate` from 0.01 to 0.1. From the initial search the optimal parameters were `dim` = 50 and `lrate` = 0.1. Since these values were on the edge of the grid, we performed another search with `dim` varying from 50 - 100 and `lrate` from 0.1 to 0.5. The optimal parameters found were `dim` = 75 and `lrate` = 0.1. The model will be retrained with these optimal hyperparameters.

```{r, echo=FALSE}
# Retrain the model with the optimal hyperparameters
rs$train(reco.train, opts = opts)
```

### Model Evaluation
With our models set up, we can do some testing. We start by creating the ratings matrix. We will then make predictions on the test set. We will evaluate the accuracy of the matrix factorisation model using RMSE. We will then apply regularisation to the model and evaluate the accuracy again.
```{r, echo=FALSE}
# Create ratings matrix
ratings.matrix <- create.ratings.matrix(comb.ratings)

# Prediction
mf.predict <- rs$predict(reco.test)
```

```{r, echo=FALSE}
# Calculate RMSE
mf.rmse.no.reg <- sqrt(mean((mf.predict - test.data$Book.Rating)^2))
mf.rmse.no.reg
```

### Regularisation

The RMSE of the matrix factorisation model without regularisation is `r round(mf.rmse.no.validation, 2)`. This tells us that on average, the model's predictions are off by `r round(mf.rmse.no.validation, 2)` units. We will now apply regularisation to the model and evaluate the accuracy again.

```{r, echo=FALSE}
rs.reg <- Reco()

rs.reg$train(reco.train, opts = list(
  dim = 75,
  lrate = 0.1,
  nmf = T,
  niter = 50,
  verbose = F,
  nthread = 4,
  costp_l2 = 0.1,
  costq_l2 = 0.1
))
```

```{r}
# Predict and calculate RMSE
mf.predict.reg <- rs.reg$predict(reco.test)
mf.rmse.reg <- sqrt(mean((mf.predict - test.data$Book.Rating)^2))
mf.rmse.reg
```

With regularisation, the RMSE of the matrix factorisation model is `r round(mf.rmse.reg, 2)`. This tells us that on average, the model's predictions are off by `r round(mf.rmse.reg, 2)` units. Compared to the RMSE of the matrix factorisation model without regularisation, the regularised model is slightly less accurate. This could be due to the fact that the regularisation parameter is too high. More sophisticated methods could be used to find the optimal regularisation parameter.

## Ensemble Model

We will now create an ensemble model that combines the user-based collaborative filtering, item-based collaborative filtering, and matrix factorisation models. We will then evaluate the accuracy of the ensemble model using RMSE. The ensemble model will be created by averaging the predictions of the three models.
```{r, echo=FALSE}
ensemble.predict <- function(user.id, isbn, r.matrix = ratings.matrix) {
  ubcf.prediction <- predict.rating.ubcf(user.id, isbn, r.matrix)
  ibcf.prediction <- predict.rating.ibcf(user.id, isbn, r.matrix)
  mf.prediction <- rs$predict(data_memory(user_index = user.id, item_index = isbn))
  
  return((ubcf.prediction + ibcf.prediction + mf.prediction) / 3)
}
```


```{r, echo=FALSE}
# Evaluate the accuracy of the ensemble model using RMSE.
# Commenting out as it takes a long time to run.
# ensemble.predictions <- mapply(ensemble.predict, test.data$User.ID, test.data$ISBN) 
# ensemble.rmse <- sqrt(mean((ensemble.predictions - test.data$Book.Rating)^2))
# ensemble.rmse
```
The RMSE of the ensemble model is 2.93. This tells us that on average, the model's predictions are off by 2.93 units. Compared to the RMSE of the matrix factorisation model of 1.84, the ensemble model is less accurate. This could be due to the fact that the ensemble model is a simple average of the three models. More sophisticated ensemble methods could be used to improve the accuracy of the ensemble model.

# Conclusion
In this project, we built a book recommendation system using collaborative filtering and matrix factorisation. We started by loading the data and performing some data pre-processing. We then built a user-based collaborative filtering system, an item-based collaborative filtering system, and a matrix factorisation system. We evaluated the accuracy of the models using RMSE. Finally, we created an ensemble model that combines the user-based collaborative filtering, item-based collaborative filtering, and matrix factorisation models. The ensemble model improved the accuracy of the predictions.