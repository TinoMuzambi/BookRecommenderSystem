---
title: "Book Recommender System"
author: "Tino Muzambi"
format: 
  html:
    embed-resources: true
    page-layout: full
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
# Libraries
install.if.missing <- function(package.name) {
  if (!require(package.name, character.only = TRUE)) {
    install.packages(package.name)
    library(package.name, character.only = TRUE)
  }
}

install.if.missing("tidyverse")
```

# The Data

The data we will be working with is a partially preprocessed version of the ”Book-Crossing” dataset. The data can be downloaded [here](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/) from Kaggle. It was collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the Book-Crossing community with kind permission from Ron Hornbaker, CTO of Humankind Systems. It contains 278 858 users (anonymised but with demographic information) providing 1 149 780 ratings (explicit/implicit) about 271 379 books.

```{r}
# Load the data
ratings <- read.csv("data/Ratings.csv")
books <- read.csv("data/Books.csv")
users <- read.csv("data/Users.csv")

head(ratings)
head(books)
head(users)
```

# Data Pre-processing

We will merge the data into a single dataframe then perform various functions to reduce the number of observations for computational efficiency. We will also perform some data cleaning.
```{r}
# Merge the data
comb.ratings <- ratings %>% 
  left_join(books, by = "ISBN") %>% 
  left_join(users, by = "User.ID")
rm(ratings, books, users)
```

Explore the structure of the combined data.

```{r}
str(comb.ratings)

summary(comb.ratings)
```

Looking at the data, we identify some cleaning that needs to be done. We will do the following to clean up the data:

-   Remove the columns that we do not need

-   Replace ages less than 13 and greater than 100 with NA

-   Convert the data types of the columns to the appropriate types

-   Replace year of publication with NA where it is before 1900

-   Remove any rows with missing values

```{r}
# Perform data cleaning
comb.ratings <- comb.ratings %>% 
  select(c("User.ID", "ISBN", "Book.Rating", "Year.Of.Publication", "Age")) %>% 
  mutate(Age = ifelse(Age < 13 | Age > 100, NA, Age)) %>% 
  mutate(Year.Of.Publication = ifelse(Year.Of.Publication < 1900, NA, Year.Of.Publication)) %>% 
  mutate(Year.Of.Publication = as.numeric(Year.Of.Publication)) %>% 
  drop_na()
summary(comb.ratings)
```

Preprocess the data to remove users with less than 5 ratings and books with less than 5 ratings
```{r}
preprocess.data <- function(ratings, min.ratings = 5) {
  user.counts <- table(ratings$User.ID)
  valid.users <- names(user.counts[user.counts >= min.ratings])
  
  filtered.ratings <- ratings %>%
    filter(User.ID %in% valid.users) %>%
    group_by(ISBN) %>%
    filter(n() >= min.ratings) %>%
    ungroup()
  
  return(filtered.ratings)
}
```

# Exploratory Data Analysis
We will now perform some exploratory data analysis to understand the data better.

```{r}
# Plot the distribution of ratings, age and year of publication faceted
comb.ratings %>% 
  gather(key = "key", value = "value", Book.Rating, Age, Year.Of.Publication) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Key Variables", x = "Value", y = "Count") +
  facet_wrap(~key, scales = "free") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```
We observe a large number of zero ratings. This can be attributed to the fact that a majority of the users posting reviews are doing so because they had a bad experience. Alternatively the users, or the site the data is derived from, could be configured to automatically input a 0 for the book rating. Before building the recommendation system, we will only keep the columns we need for the analysis. We will also then take a sample of 50 000 ratings for computational efficiency.
```{r}
comb.ratings <- preprocess.data(comb.ratings)

# Select the columns we need
comb.ratings <- comb.ratings %>% 
  select(User.ID, ISBN, Book.Rating)

# Sample 35 000 ratings
set.seed(50731)
comb.ratings <- comb.ratings[sample(nrow(comb.ratings), 35000),]
```

# Recommendation Systems
We will now begin building our recommendation system. We will start by building a user-based collaborative filtering system. We will then build an item-based collaborative filtering system. Finally, we will build a matrix factorisation system.

## User-Based Collaborative Filtering
We will start by building a user-based collaborative filtering system. We will first create a user-item matrix then calculate the cosine similarity between users. We will then use the similarity matrix to predict the ratings for the books.
```{r}
cosine.similarity <- function(user1, user2) {
  max.len <- max(length(user1), length(user2))
  user1.padded <- c(user1, rep(0, max.len - length(user1)))
  user2.padded <- c(user2, rep(0, max.len - length(user2)))
  similarity <- sum(user1.padded * user2.padded) / (sqrt(sum(user1.padded^2)) * sqrt(sum(user2.padded^2)))
  return(similarity)
}
```

```{r}
predict.rating.ubcf <- function(user.id, isbn, k = 10) {
  # Find k most similar users
  neighbors <- comb.ratings[comb.ratings$User.ID != user.id, ]
  similarities <- sapply(neighbors$User.ID, function(x) cosine.similarity(user.id, x))
  nearest.neighbors <- neighbors[order(similarities, decreasing = TRUE)[1:k], ]
  
  # Calculate weighted average rating from neighbors
  weighted.ratings <- nearest.neighbors$Book.Rating * similarities[order(similarities, decreasing = TRUE)[1:k]]
  predicted.rating <- sum(weighted.ratings) / sum(similarities[order(similarities, decreasing = TRUE)[1:k]])
  
  return(predicted.rating)
}
```

## Item-Based Collaborative Filtering
We will now build an item-based collaborative filtering system. We will first create an item-user matrix then calculate the cosine similarity between items. We will then use the similarity matrix to predict the ratings for the books.

First, we need a cosine similarity function for items.
```{r}
cosine.similarity.items <- function(item1, item2) {
  max.len <- max(length(item1), length(item2))
  item1.padded <- c(item1, rep(0, max.len - length(item1)))
  item2.padded <- c(item2, rep(0, max.len - length(item2)))
  similarity <- sum(item1.padded * item2.padded) / (sqrt(sum(item1.padded^2)) * sqrt(sum(item2.padded^2)))
  return(similarity)
}
```

Next, we need a function to predict the ratings for items.
```{r}
predict.rating.ibcf <- function(user.id, isbn, k = 10) {
  # Find k most similar items
  neighbors <- comb.ratings[comb.ratings$ISBN != isbn, ]
  similarities <- sapply(neighbors$ISBN, function(x) cosine.similarity.items(comb.ratings[comb.ratings$ISBN == isbn, ]$Book.Rating, comb.ratings[comb.ratings$ISBN == x, ]$Book.Rating))
  nearest.neighbors <- neighbors[order(similarities, decreasing = TRUE)[1:k], ]
  
  # Calculate weighted average rating from neighbors
  weighted.ratings <- nearest.neighbors$Book.Rating * similarities[order(similarities, decreasing = TRUE)[1:k]]
  predicted.rating <- sum(weighted.ratings) / sum(similarities[order(similarities, decreasing = TRUE)[1:k]])
  
  return(predicted.rating)
}
```

## Matrix Factorisation
We will now build a matrix factorisation system. We will first create a user-item matrix then factorise the matrix using singular value decomposition. We will then use the factorised matrix to predict the ratings for the books.
```{r}
predict.rating.mf <- function(user.id, isbn) {
  # Create user-item matrix
  user.item.matrix <- comb.ratings %>% 
    spread(key = ISBN, value = Book.Rating) %>% 
    column_to_rownames(var = "User.ID")
  
  # Factorise the matrix
  svd.matrix <- svd(user.item.matrix)
  u <- svd.matrix$u
  d <- diag(svd.matrix$d)
  v <- svd.matrix$v
  
  # Predict the rating
  predicted.rating <- u[user.id, ] %*% d %*% v[, isbn]
  
  return(predicted.rating)
}
```

Let's do some predictions
```{r}
predict.rating.ubcf(155141, "059035342X")
predict.rating.ibcf(155141, "059035342X")
predict.rating.mf(155141, "059035342X")
```


```{r}
# Split data into training and testing sets
# set.seed(123)
# train.index <- sample(1:nrow(comb.ratings), 0.8 * nrow(comb.ratings))
# train.data <- comb.ratings[train.index, ]
# test.data <- comb.ratings[-train.index, ]
# 
# # Make predictions on the test set
# test.data$predicted.rating <- sapply(1:nrow(test.data), function(i) {
#   user.id <- test.data$User.ID[i]
#   isbn <- test.data$ISBN[i]
#   predict.rating.ubcf(user.id, isbn)
# })
# 
# # Calculate RMSE
# rmse <- sqrt(mean((test.data$Book.Rating - test.data$predicted.rating)^2))
# print(paste("RMSE:", rmse))
```